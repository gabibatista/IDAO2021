{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "idao-final.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZElCwsauHJLd"
      },
      "source": [
        "# imports\n",
        "import statistics\n",
        "import re\n",
        "import keras\n",
        "\n",
        "import keras.backend as K\n",
        "import numpy as np\n",
        "\n",
        "from google.colab import files\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "from collections import Counter\n",
        "from PIL import Image\n",
        "from matplotlib.image import imread\n",
        "from matplotlib import pyplot as plt\n",
        "from random import shuffle\n",
        "from math import ceil\n",
        "from scipy import ndimage\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "# from sklearn.model_selection import KFold\n",
        "\n",
        "from keras.models import Model\n",
        "from keras.models import load_model\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Input, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D, MaxPooling2D, Dropout\n",
        "from keras.utils import plot_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i-CEhkXIvZ9k"
      },
      "source": [
        "# variaveis\n",
        "\n",
        "img_to_load = 1000 # imagens a serem carregadas por vez\n",
        "crop_size = 200 # altura e largura da imagem a ser enviada para o modelo\n",
        "train = 1 # porcentagem de dados a ser usado para treino"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2qRqIIexPnmW"
      },
      "source": [
        "#função para pegar só o centro da imagem\n",
        "\n",
        "def crop_center(img,cropx,cropy):\n",
        "    y,x = img.shape\n",
        "    startx = x//2-(cropx//2)\n",
        "    starty = y//2-(cropy//2)    \n",
        "    return img[starty:starty+cropy,startx:startx+cropx]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d22g_nlGPqon"
      },
      "source": [
        "#carregar nome de todas as imagens\n",
        "\n",
        "ER_path = '/gdrive/My Drive/IDAO/train/ER/'\n",
        "ER_files = [f'/gdrive/My Drive/IDAO/train/ER/{f}' for f in listdir(ER_path) if isfile(join(ER_path, f))]\n",
        "\n",
        "NR_path = '/gdrive/My Drive/IDAO/train/NR/'\n",
        "NR_files = [f'/gdrive/My Drive/IDAO/train/NR/{f}' for f in listdir(NR_path) if isfile(join(NR_path, f))]\n",
        "\n",
        "all_files = ER_files + NR_files\n",
        "shuffle(all_files)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ljVRW68UP4Fj"
      },
      "source": [
        "# criar um dicionário para armazenar todos os dados separadamente\n",
        "# isso é necessário para garantir que 70% de cada uma das classes serão usadas para treino\n",
        "\n",
        "images = {'NR_1_keV': [], 'NR_6_keV': [], 'NR_20_keV': [], 'ER_3_keV': [], 'ER_10_keV': [], 'ER_30_keV': []}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eBvmn8kHCrw1"
      },
      "source": [
        "# carrega 70% dos dados no X_train e 30% no X_test\n",
        "\n",
        "X_train = []\n",
        "X_test = []\n",
        "\n",
        "for particle_type in images.keys():\n",
        "    X_train = X_train + images[particle_type][:int(train*len(images[particle_type]))]\n",
        "    X_test = X_test + images[particle_type][int(train*len(images[particle_type])):]\n",
        "shuffle(X_train)\n",
        "shuffle(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WFKK1q9IJjby"
      },
      "source": [
        "# cria o Y_train e Y_test com base no X_train e X_test criado anteriormente\n",
        "\n",
        "Y_train_r = [int(re.findall(r\"\\d+(?=_keV)\", en)[0])/30.0 for en in X_train]\n",
        "Y_test_r = [int(re.findall(r\"\\d+(?=_keV)\", en)[0])/30.0 for en in X_test]\n",
        "Y_train_c = [(1 if '_NR_' in x else 0) for x in X_train]\n",
        "Y_test_c = [(1 if '_NR_' in x else 0) for x in X_test]\n",
        "\n",
        "#for i in range(10):\n",
        "#  print(f\"{Y_train_c[i]} {Y_train_r[i]} {X_train[i]}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SKzwTkEO1mEA"
      },
      "source": [
        "def create_model_v0():\n",
        "    inputs = Input(shape=(crop_size, crop_size, 1), name='input')\n",
        "\n",
        "    x = Conv2D(32, (2, 2))(inputs)\n",
        "    x = Activation('relu')(x)\n",
        "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
        "\n",
        "    x = Conv2D(64, (3, 3))(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
        "\n",
        "    x = Conv2D(64, (3, 3))(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
        "\n",
        "    x = Flatten()(x)\n",
        "\n",
        "    x = Dense(128)(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = Dense(64)(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = Dense(32)(x)\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "    output1 = Dense(1)(x)\n",
        "    output1 = Activation('sigmoid', name='classification')(output1)\n",
        "\n",
        "    output2 = Dense(1)(x)\n",
        "    output2 = Activation('linear', name='regression')(output2)\n",
        "\n",
        "    model = Model(inputs=inputs, outputs=[output1, output2])\n",
        "    opt = keras.optimizers.Adam(learning_rate=0.0005)\n",
        "    model.compile(loss={'classification': 'binary_crossentropy', \n",
        "                        'regression': 'mean_absolute_error'},\n",
        "                  optimizer=opt,\n",
        "                  metrics={'classification': 'accuracy',\n",
        "                           'regression': 'mean_squared_error'})\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mcwEwp6Si5iA"
      },
      "source": [
        "def create_model_v1():\n",
        "    inputs = Input(shape=(crop_size, crop_size, 1), name='input')\n",
        "\n",
        "    x = Conv2D(64, (2, 2))(inputs)\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "    x = Conv2D(64, (3, 3))(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
        "\n",
        "    x = Conv2D(64, (3, 3))(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
        "\n",
        "    x = Flatten()(x)\n",
        "\n",
        "    x = Dense(128)(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = Dense(64)(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = Dense(32)(x)\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "    output1 = Dense(1)(x)\n",
        "    output1 = Activation('sigmoid', name='classification')(output1)\n",
        "\n",
        "    output2 = Dense(1)(x)\n",
        "    output2 = Activation('linear', name='regression')(output2)\n",
        "\n",
        "    model = Model(inputs=inputs, outputs=[output1, output2])\n",
        "    opt = keras.optimizers.Adam(learning_rate=0.0005)\n",
        "    model.compile(loss={'classification': 'binary_crossentropy', \n",
        "                        'regression': 'mean_absolute_error'},\n",
        "                  optimizer=opt,\n",
        "                  metrics={'classification': 'accuracy',\n",
        "                           'regression': 'mean_squared_error'})\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Ar5RAFEi8CR"
      },
      "source": [
        "def create_model_v2():\n",
        "    inputs = Input(shape=(crop_size, crop_size, 1), name='input')\n",
        "\n",
        "    x = Conv2D(32, (2, 2))(inputs)\n",
        "    x = Activation('relu')(x)\n",
        "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
        "\n",
        "    x = Conv2D(64, (3, 3))(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
        "\n",
        "    x = Conv2D(128, (3, 3))(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = MaxPooling2D(pool_size=(3, 3))(x)\n",
        "\n",
        "    x = Flatten()(x)\n",
        "\n",
        "    x = Dense(128)(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = Dense(128)(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = Dense(64)(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = Dense(32)(x)\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "    output1 = Dense(1)(x)\n",
        "    output1 = Activation('sigmoid', name='classification')(output1)\n",
        "\n",
        "    output2 = Dense(1)(x)\n",
        "    output2 = Activation('linear', name='regression')(output2)\n",
        "\n",
        "    model = Model(inputs=inputs, outputs=[output1, output2])\n",
        "    opt = keras.optimizers.Adam(learning_rate=0.0005)\n",
        "    model.compile(loss={'classification': 'binary_crossentropy', \n",
        "                        'regression': 'mean_absolute_error'},\n",
        "                  optimizer=opt,\n",
        "                  metrics={'classification': 'accuracy',\n",
        "                           'regression': 'mean_squared_error'})\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uS0LAh53TsVU"
      },
      "source": [
        "X_train_img = []\n",
        "for i in range(ceil(len(X_train)/img_to_load)):\n",
        "    begin = i*img_to_load\n",
        "    limit = min(i*img_to_load + img_to_load, len(X_train))\n",
        "    if begin == limit:\n",
        "        break\n",
        "    for file in X_train[begin:limit]:\n",
        "        im = imread(file)\n",
        "        # X_train_img.append(crop_center(im, crop_size, crop_size))\n",
        "        np_img = np.asarray(crop_center(im, crop_size, crop_size)).reshape((1,150,150,1))\n",
        "        if len(X_train_img):\n",
        "          X_train_img = np.append(X_train_img, np_img, axis=0)\n",
        "        else:\n",
        "          X_train_img = np_img\n",
        "    print(f'loaded {begin} to {limit}')\n",
        "\n",
        "Y_train_img_c = np.stack(Y_train_c, axis=0).reshape((len(Y_train_c), 1))\n",
        "Y_train_img_r = np.stack(Y_train_r, axis=0).reshape((len(Y_train_r), 1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zkLvv9r4lwVZ"
      },
      "source": [
        "X_train_img = X_train_img.astype(np.float16)\n",
        "Y_train_img_c = Y_train_img_c.astype(np.float16)\n",
        "Y_train_img_r = Y_train_img_r.astype(np.float16)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ML78rBSwvXJ7"
      },
      "source": [
        "X_train_img.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ONvC_0J8ULz"
      },
      "source": [
        "# preds 1=c_loss, 2=r_loss, 3=c_acc, 4=r_mse\n",
        "all_losses = []\n",
        "all_c_losses = []\n",
        "all_r_losses = []\n",
        "all_c_accs = []\n",
        "all_r_mses = []\n",
        "\n",
        "n_folds = 5\n",
        "models = 3\n",
        "for m_num in range(models):\n",
        "  loss = []\n",
        "  c_loss = []\n",
        "  r_loss = []\n",
        "  c_acc = []\n",
        "  r_mse = []\n",
        "  for i in range(n_folds):\n",
        "      print(f\"Model {m_num} Training on Fold: {i+1}\")\n",
        "      t_x, val_x, t_y_c, val_y_c, t_y_r, val_y_r = train_test_split(X_train_img, Y_train_img_c, Y_train_img_r, test_size=0.3, random_state = np.random.randint(1,1000, 1)[0])\n",
        "      model = eval(f'create_model_v{m_num}()')\n",
        "      model.fit(t_x, {'classification': t_y_c, 'regression': t_y_r}, epochs=10, batch_size=64)\n",
        "      preds = model.evaluate(val_x, {'classification': val_y_c, 'regression': val_y_r}, batch_size=32, verbose=1, sample_weight=None)\n",
        "      loss.append(preds[0])\n",
        "      c_loss.append(preds[1])\n",
        "      r_loss.append(preds[2])\n",
        "      c_acc.append(preds[3])\n",
        "      r_mse.append(preds[4])\n",
        "  all_losses.append(loss)\n",
        "  all_c_losses.append(c_loss)\n",
        "  all_r_losses.append(r_loss)\n",
        "  all_c_accs.append(c_acc)\n",
        "  all_r_mses.append(r_mse)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LT4NeRqjFC-c"
      },
      "source": [
        "# == Provide average scores ==\n",
        "for m in range(models):\n",
        "  print(f'Model {m}')\n",
        "  # print('------------------------------------------------------------------------')\n",
        "  print('Score per fold')\n",
        "  for f in range(n_folds):\n",
        "    # print('------------------------------------------------------------------------')\n",
        "    print(f'> Fold {f+1} - loss: {round(all_losses[m][f], 4)} - c_loss: {round(all_c_losses[m][f], 4)} - r_loss: {round(all_r_losses[m][f], 4)} - c_acc: {round(all_c_accs[m][f], 4)} - r_mse: {round(all_r_mses[m][f], 4)}')\n",
        "  # print('------------------------------------------------------------------------')\n",
        "  print('Average scores for all folds:')\n",
        "  print(f'> avg loss: {round(np.mean(all_losses[m]), 4)} (+- {round(np.std(all_losses[m]), 4)})')\n",
        "  print(f'> avg c_loss: {round(np.mean(all_c_losses[m]), 4)} (+- {round(np.std(all_c_losses[m]), 4)})')\n",
        "  print(f'> avg r_loss: {round(np.mean(all_r_losses[m]), 4)} (+- {round(np.std(all_r_losses[m]), 4)})')\n",
        "  print(f'> avg c_acc: {round(np.mean(all_c_accs[m]), 4)} (+- {round(np.std(all_c_accs[m]), 4)})')\n",
        "  print(f'> avg r_mse: {round(np.mean(all_r_mses[m]), 4)} (+- {round(np.std(all_r_mses[m]), 4)})')\n",
        "  print('------------------------------------------------------------------------')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jV-Kcxzzt03H"
      },
      "source": [
        "# carrega as imagens usadas para testar \n",
        "X_test_img = []\n",
        "for file in X_test:\n",
        "    im = imread(file)\n",
        "    np_img = np.asarray(crop_center(im, crop_size, crop_size)).reshape((1,150,150,1))\n",
        "    if len(X_test_img):\n",
        "      X_test_img = np.append(X_test_img, np_img, axis=0)\n",
        "    else:\n",
        "      X_test_img = np_img\n",
        "\n",
        "print(f'loaded test images')\n",
        "\n",
        "Y_test_img_c = Y_test_c\n",
        "Y_test_img_c = np.stack(Y_test_img_c, axis=0).reshape((len(Y_test_img_c), 1))\n",
        "\n",
        "Y_test_img_r = Y_test_r\n",
        "Y_test_img_r = np.stack(Y_test_img_r, axis=0).reshape((len(Y_test_img_r), 1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WftYTnFa6lbG"
      },
      "source": [
        "!pip install optuna"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zOcL4d_XtJRy"
      },
      "source": [
        "import optuna\n",
        "from keras.backend import clear_session"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wjjw5GTTMOf8"
      },
      "source": [
        "def objective(trial):\n",
        "    # Clear clutter from previous Keras session graphs.\n",
        "    # K.clear_session()\n",
        "    clear_session()\n",
        "\n",
        "    inputs = Input(shape=(crop_size, crop_size, 1), name='input')\n",
        "\n",
        "    x = Conv2D(filters=trial.suggest_categorical(\"filters1\", [32, 64]), kernel_size=trial.suggest_categorical(\"kernel_size1\", [2, 3, 4]))(inputs)\n",
        "    x = Activation(activation=trial.suggest_categorical(\"activation1\", [\"relu\", \"linear\", \"sigmoid\"]))(x)\n",
        "    x = MaxPooling2D(pool_size=trial.suggest_categorical(\"pool_size1\", [2, 3]))(x)\n",
        "\n",
        "    x = Conv2D(filters=trial.suggest_categorical(\"filters2\", [32, 64]), kernel_size=trial.suggest_categorical(\"kernel_size2\", [2, 3, 4]))(x)\n",
        "    x = Activation(activation=trial.suggest_categorical(\"activation2\", [\"relu\", \"linear\", \"sigmoid\"]))(x)\n",
        "    x = MaxPooling2D(pool_size=trial.suggest_categorical(\"pool_size2\", [2, 3]))(x)\n",
        "\n",
        "    x = Conv2D(filters=trial.suggest_categorical(\"filters3\", [32, 64]), kernel_size=trial.suggest_categorical(\"kernel_size3\", [2, 3, 4]))(x)\n",
        "    x = Activation(activation=trial.suggest_categorical(\"activation3\", [\"relu\", \"linear\", \"sigmoid\"]))(x)\n",
        "    x = MaxPooling2D(pool_size=trial.suggest_categorical(\"pool_size3\", [2, 3]))(x)\n",
        "\n",
        "\n",
        "    x = Flatten()(x)\n",
        "\n",
        "    x = Dense(units=trial.suggest_categorical(\"units1\", [128, 64]))(x)\n",
        "    x = Activation(trial.suggest_categorical(\"activation4\", [\"relu\", \"linear\", \"sigmoid\"]))(x)\n",
        "    x = Dense(units=trial.suggest_categorical(\"units2\", [128, 64, 32]))(x)\n",
        "    x = Activation(trial.suggest_categorical(\"activation5\", [\"relu\", \"linear\", \"sigmoid\"]))(x)\n",
        "    x = Dense(units=trial.suggest_categorical(\"units3\", [64, 32]))(x)\n",
        "    x = Activation(trial.suggest_categorical(\"activation6\", [\"relu\", \"linear\", \"sigmoid\"]))(x)\n",
        "\n",
        "    output1 = Dense(1)(x)\n",
        "    output1 = Activation('sigmoid', name='classification')(output1)\n",
        "\n",
        "    output2 = Dense(1)(x)\n",
        "    output2 = Activation('linear', name='regression')(output2)\n",
        "\n",
        "    model = Model(inputs=inputs, outputs=[output1, output2])\n",
        "    lr = trial.suggest_float(\"lr\", 1e-5, 1e-3, log=True)\n",
        "    opt = keras.optimizers.Adam(learning_rate=lr)\n",
        "    model.compile(loss={'classification': 'binary_crossentropy', \n",
        "                        'regression': 'mean_absolute_error'},\n",
        "                  optimizer=opt,\n",
        "                  metrics={'classification': 'accuracy',\n",
        "                           'regression': 'mean_squared_error'})\n",
        "\n",
        "    t_x, val_x, t_y_c, val_y_c, t_y_r, val_y_r = train_test_split(X_train_img, Y_train_img_c, Y_train_img_r, test_size=0.3, random_state = np.random.randint(1,1000, 1)[0])\n",
        "\n",
        "    model.fit(t_x,{'classification': t_y_c, 'regression': t_y_r},\n",
        "    # model.fit(X_train_img,{'classification': Y_train_img_c, 'regression': Y_train_img_r},\n",
        "        #validation_data=(x_valid, y_valid),\n",
        "        shuffle=True,\n",
        "        batch_size=64,\n",
        "        epochs=10,\n",
        "        verbose=False,\n",
        "    )\n",
        "\n",
        "    # Evaluate the model accuracy on the validation set.\n",
        "    score = model.evaluate(val_x, {'classification': val_y_c, 'regression': val_y_r}, batch_size=32, verbose=1)\n",
        "    loss = score[0]\n",
        "    # r_loss = score[2]\n",
        "    # c_acc = score[3]\n",
        "    return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xTWIOqM3nDM9"
      },
      "source": [
        "#gc_after_trial=True\n",
        "import gc\n",
        "gc.collect()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "amGHOAMTtZar"
      },
      "source": [
        "study = optuna.create_study(direction=\"minimize\")\n",
        "#study.optimize(objective, n_trials=30, gc_after_trial=True)\n",
        "\n",
        "study.optimize(objective, n_trials=300,  gc_after_trial=True, callbacks=[lambda study, trial: gc.collect()])\n",
        "# study = optuna.create_study(directions=[\"minimize\", \"maximize\"])\n",
        "# study = optuna.multi_objective.create_study([\"minimize\", \"maximize\"])\n",
        "#optuna.create_study(directions=[\"minimize\", \"maximize\"])\n",
        "#study.best_params"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tkaNCGDnypcY"
      },
      "source": [
        "study.best_trial"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BioFllNTCQc-"
      },
      "source": [
        "study.best_trial.params"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BX_73agZHPnE"
      },
      "source": [
        "fig = optuna.visualization.plot_optimization_history(study)\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7AoyKzNmT_Ng"
      },
      "source": [
        "fig = optuna.visualization.plot_param_importances(study)\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MviUApOvw9d4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        },
        "outputId": "89042218-2eb5-4e82-f50b-29e0a4f49f64"
      },
      "source": [
        "while True: pass"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-b16dc615ea65>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}